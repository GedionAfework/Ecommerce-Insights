{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 4: Sentiment Modeling\n",
    "\n",
    "This notebook implements sentiment classification models to predict review sentiment from text and metadata features.\n",
    "\n",
    "## Objectives\n",
    "1. Prepare data for sentiment modeling\n",
    "2. Implement baseline models (Logistic Regression, Naive Bayes, SVM, Random Forest)\n",
    "3. Engineer advanced text features\n",
    "4. Build advanced models (XGBoost, Neural Networks, LSTM, BERT)\n",
    "5. Perform hyperparameter tuning\n",
    "6. Evaluate and compare all models\n",
    "7. Select and save best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score\n",
    ")\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Project paths\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\" / \"fused\"\n",
    "REPORTS_DIR = PROJECT_ROOT / \"reports\"\n",
    "FIGURES_DIR = REPORTS_DIR / \"figures\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "FIGURES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Models directory: {MODELS_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fused dataset (using similar approach to EDA, but larger sample for modeling)\n",
    "import pyarrow.parquet as pq\n",
    "import gc\n",
    "\n",
    "fused_file = PROCESSED_DIR / \"books_books_fused.parquet\"\n",
    "\n",
    "print(f\"Loading dataset from: {fused_file}\")\n",
    "\n",
    "# For modeling, we'll use a larger sample (100K rows) for better model performance\n",
    "if fused_file.exists():\n",
    "    parquet_file = pq.ParquetFile(fused_file)\n",
    "    num_rows = parquet_file.metadata.num_rows\n",
    "    num_row_groups = parquet_file.num_row_groups\n",
    "    \n",
    "    print(f\"Total rows in file: {num_rows:,}\")\n",
    "    print(f\"Number of row groups: {num_row_groups}\")\n",
    "    \n",
    "    # Sample size for modeling (larger than EDA for better model performance)\n",
    "    SAMPLE_SIZE = 100_000\n",
    "    \n",
    "    if num_rows > SAMPLE_SIZE:\n",
    "        print(f\"\\nSampling {SAMPLE_SIZE:,} rows for modeling...\")\n",
    "        \n",
    "        # Try to read multiple row groups\n",
    "        max_row_groups_to_try = min(20, num_row_groups)\n",
    "        rng = np.random.RandomState(42)\n",
    "        row_groups_to_try = sorted(rng.choice(num_row_groups, \n",
    "                                              size=max_row_groups_to_try, \n",
    "                                              replace=False))\n",
    "        \n",
    "        print(f\"Attempting to read from {len(row_groups_to_try)} row groups...\")\n",
    "        batches = []\n",
    "        \n",
    "        for rg_idx in row_groups_to_try:\n",
    "            try:\n",
    "                batch = parquet_file.read_row_groups([rg_idx]).to_pandas()\n",
    "                \n",
    "                # Handle dictionary columns\n",
    "                for col in batch.columns:\n",
    "                    if batch[col].dtype == 'object':\n",
    "                        sample_vals = batch[col].dropna().head(5)\n",
    "                        if len(sample_vals) > 0 and any(isinstance(val, dict) for val in sample_vals):\n",
    "                            batch[col] = batch[col].astype(str)\n",
    "                \n",
    "                batches.append(batch)\n",
    "                \n",
    "                total_rows = sum(len(b) for b in batches)\n",
    "                if total_rows >= SAMPLE_SIZE * 1.5:\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not read row group {rg_idx}: {str(e)[:100]}\")\n",
    "                continue\n",
    "        \n",
    "        if batches:\n",
    "            df_temp = pd.concat(batches, ignore_index=True)\n",
    "            \n",
    "            if len(df_temp) > SAMPLE_SIZE:\n",
    "                df = df_temp.sample(n=SAMPLE_SIZE, random_state=42)\n",
    "            else:\n",
    "                df = df_temp.copy()\n",
    "            \n",
    "            del batches, df_temp\n",
    "            gc.collect()\n",
    "        else:\n",
    "            raise ValueError(\"Could not read any data from parquet file\")\n",
    "    else:\n",
    "        df = pd.read_parquet(fused_file)\n",
    "    \n",
    "    # Ensure helpfulness_ratio is numeric\n",
    "    if 'helpfulness_ratio' in df.columns:\n",
    "        df['helpfulness_ratio'] = pd.to_numeric(df['helpfulness_ratio'], errors='coerce')\n",
    "    \n",
    "    print(f\"\\nDataset loaded successfully!\")\n",
    "    print(f\"   Shape: {df.shape}\")\n",
    "    print(f\"   Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"File not found: {fused_file})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation and Target Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available columns\n",
    "print(\"Available columns:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"  {i:2d}. {col}\")\n",
    "\n",
    "# Check for review text column\n",
    "text_cols = [col for col in df.columns if 'review' in col.lower() and 'text' in col.lower()]\n",
    "print(f\"\\nReview text columns found: {text_cols}\")\n",
    "\n",
    "# Check data quality\n",
    "print(f\"\\nDataset info:\")\n",
    "print(f\"  Total rows: {len(df):,}\")\n",
    "print(f\"  Missing values: {df.isnull().sum().sum():,}\")\n",
    "if 'overall' in df.columns:\n",
    "    print(f\"  Rating distribution:\")\n",
    "    print(df['overall'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentiment labels from ratings\n",
    "if 'overall' in df.columns:\n",
    "    # Multi-class: 5 classes (1-5 stars)\n",
    "    df['sentiment_5class'] = df['overall'].astype(int)\n",
    "    \n",
    "    # 3-class: Positive, Neutral, Negative\n",
    "    df['sentiment_3class'] = df['overall'].apply(\n",
    "        lambda x: 'Positive' if x >= 4 else ('Neutral' if x == 3 else 'Negative')\n",
    "    )\n",
    "    \n",
    "    # Binary: Positive vs Negative\n",
    "    df['sentiment_binary'] = df['overall'].apply(\n",
    "        lambda x: 'Positive' if x >= 4 else 'Negative'\n",
    "    )\n",
    "    \n",
    "    print(\"Sentiment distribution (3-class):\")\n",
    "    print(df['sentiment_3class'].value_counts())\n",
    "    print(f\"\\nSentiment distribution (binary):\")\n",
    "    print(df['sentiment_binary'].value_counts())\n",
    "    print(f\"\\nSentiment distribution (5-class):\")\n",
    "    print(df['sentiment_5class'].value_counts().sort_index())\n",
    "else:\n",
    "    raise ValueError(\"Rating column 'overall' not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select target (we'll use 3-class for main analysis)\n",
    "TARGET = 'sentiment_3class'\n",
    "\n",
    "# Get review text (use first available text column)\n",
    "if text_cols:\n",
    "    TEXT_COL = text_cols[0]\n",
    "    print(f\"Using text column: {TEXT_COL}\")\n",
    "else:\n",
    "    # Try to find any text-like column\n",
    "    TEXT_COL = None\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object' and df[col].str.len().mean() > 50:\n",
    "            TEXT_COL = col\n",
    "            print(f\"Using text column: {TEXT_COL}\")\n",
    "            break\n",
    "\n",
    "if TEXT_COL is None:\n",
    "    raise ValueError(\"No suitable text column found\")\n",
    "\n",
    "# Remove rows with missing text or target\n",
    "df_clean = df[[TEXT_COL, TARGET]].dropna()\n",
    "print(f\"\\nClean dataset shape: {df_clean.shape}\")\n",
    "print(f\"Removed {len(df) - len(df_clean)} rows with missing data\")\n",
    "\n",
    "# Check text length statistics\n",
    "if len(df_clean) > 0:\n",
    "    text_lengths = df_clean[TEXT_COL].str.len()\n",
    "    print(f\"\\nText length statistics:\")\n",
    "    print(f\"  Mean: {text_lengths.mean():.0f} characters\")\n",
    "    print(f\"  Median: {text_lengths.median():.0f} characters\")\n",
    "    print(f\"  Min: {text_lengths.min():.0f} characters\")\n",
    "    print(f\"  Max: {text_lengths.max():.0f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "X = df_clean[TEXT_COL]\n",
    "y = df_clean[TARGET]\n",
    "\n",
    "# Stratified split to maintain class distribution\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Further split training into train and validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.15, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train):,} samples\")\n",
    "print(f\"Validation set: {len(X_val):,} samples\")\n",
    "print(f\"Test set: {len(X_test):,} samples\")\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts())\n",
    "print(f\"\\nClass distribution in validation set:\")\n",
    "print(y_val.value_counts())\n",
    "print(f\"\\nClass distribution in test set:\")\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Feature Extraction (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF vectorizer\n",
    "tfidf = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    ngram_range=(1, 2),  # Unigrams and bigrams\n",
    "    min_df=2,  # Minimum document frequency\n",
    "    max_df=0.95,  # Maximum document frequency\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "# Fit and transform\n",
    "print(\"Fitting TF-IDF vectorizer...\")\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_val_tfidf = tfidf.transform(X_val)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "print(f\"TF-IDF feature matrix shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Vocabulary size: {len(tfidf.vocabulary_)}\")\n",
    "print(f\"Feature matrix is sparse: {hasattr(X_train_tfidf, 'toarray')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Model 1: Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "print(\"Training Logistic Regression...\")\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
    "lr_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred_lr = lr_model.predict(X_train_tfidf)\n",
    "y_val_pred_lr = lr_model.predict(X_val_tfidf)\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nLogistic Regression Results:\")\n",
    "print(f\"Training Accuracy: {accuracy_score(y_train, y_train_pred_lr):.4f}\")\n",
    "print(f\"Validation Accuracy: {accuracy_score(y_val, y_val_pred_lr):.4f}\")\n",
    "print(f\"Validation F1-Score (macro): {f1_score(y_val, y_val_pred_lr, average='macro'):.4f}\")\n",
    "print(f\"Validation F1-Score (weighted): {f1_score(y_val, y_val_pred_lr, average='weighted'):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_val_pred_lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Model 2: Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Naive Bayes\n",
    "print(\"Training Naive Bayes...\")\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_val_pred_nb = nb_model.predict(X_val_tfidf)\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nNaive Bayes Results:\")\n",
    "print(f\"Validation Accuracy: {accuracy_score(y_val, y_val_pred_nb):.4f}\")\n",
    "print(f\"Validation F1-Score (macro): {f1_score(y_val, y_val_pred_nb, average='macro'):.4f}\")\n",
    "print(f\"Validation F1-Score (weighted): {f1_score(y_val, y_val_pred_nb, average='weighted'):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_val_pred_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Model 3: Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM (using smaller sample for speed)\n",
    "print(\"Training SVM (this may take a while)...\")\n",
    "svm_sample_size = min(10000, X_train_tfidf.shape[0])\n",
    "svm_indices = np.random.choice(X_train_tfidf.shape[0], svm_sample_size, replace=False, random_state=42)\n",
    "\n",
    "svm_model = SVC(kernel='linear', random_state=42, probability=True)\n",
    "svm_model.fit(X_train_tfidf[svm_indices], y_train.iloc[svm_indices])\n",
    "\n",
    "# Predictions\n",
    "y_val_pred_svm = svm_model.predict(X_val_tfidf)\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nSVM Results:\")\n",
    "print(f\"Validation Accuracy: {accuracy_score(y_val, y_val_pred_svm):.4f}\")\n",
    "print(f\"Validation F1-Score (macro): {f1_score(y_val, y_val_pred_svm, average='macro'):.4f}\")\n",
    "print(f\"Validation F1-Score (weighted): {f1_score(y_val, y_val_pred_svm, average='weighted'):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_val_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Model 4: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest (using smaller sample for speed)\n",
    "print(\"Training Random Forest (this may take a while)...\")\n",
    "rf_sample_size = min(20000, X_train_tfidf.shape[0])\n",
    "rf_indices = np.random.choice(X_train_tfidf.shape[0], rf_sample_size, replace=False, random_state=42)\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1, max_depth=20)\n",
    "rf_model.fit(X_train_tfidf[rf_indices], y_train.iloc[rf_indices])\n",
    "\n",
    "# Predictions\n",
    "y_val_pred_rf = rf_model.predict(X_val_tfidf)\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\nRandom Forest Results:\")\n",
    "print(f\"Validation Accuracy: {accuracy_score(y_val, y_val_pred_rf):.4f}\")\n",
    "print(f\"Validation F1-Score (macro): {f1_score(y_val, y_val_pred_rf, average='macro'):.4f}\")\n",
    "print(f\"Validation F1-Score (weighted): {f1_score(y_val, y_val_pred_rf, average='weighted'):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_val_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare baseline models\n",
    "models = {\n",
    "    'Logistic Regression': (lr_model, y_val_pred_lr),\n",
    "    'Naive Bayes': (nb_model, y_val_pred_nb),\n",
    "    'SVM': (svm_model, y_val_pred_svm),\n",
    "    'Random Forest': (rf_model, y_val_pred_rf)\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, (model, predictions) in models.items():\n",
    "    results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy_score(y_val, predictions),\n",
    "        'F1-Macro': f1_score(y_val, predictions, average='macro'),\n",
    "        'F1-Weighted': f1_score(y_val, predictions, average='weighted'),\n",
    "        'Precision': precision_score(y_val, predictions, average='macro'),\n",
    "        'Recall': recall_score(y_val, predictions, average='macro')\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('F1-Macro', ascending=False)\n",
    "print(\"Baseline Models Comparison:\")\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(results_df))\n",
    "width = 0.15\n",
    "\n",
    "metrics = ['Accuracy', 'F1-Macro', 'F1-Weighted', 'Precision', 'Recall']\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax.bar(x + i*width, results_df[metric], width, label=metric)\n",
    "\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Baseline Models Performance Comparison')\n",
    "ax.set_xticks(x + width * 2)\n",
    "ax.set_xticklabels(results_df['Model'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'baseline_models_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Save results\n",
    "results_df.to_csv(REPORTS_DIR / 'baseline_models_results.csv', index=False)\n",
    "print(f\"\\nResults saved to: {REPORTS_DIR / 'baseline_models_results.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Baseline Models Summary\n",
    "\n",
    "We've successfully implemented and evaluated 4 baseline models:\n",
    "1. **Logistic Regression** - Linear classifier with good interpretability\n",
    "2. **Naive Bayes** - Probabilistic classifier, fast training\n",
    "3. **SVM** - Support Vector Machine with linear kernel\n",
    "4. **Random Forest** - Ensemble tree-based model\n",
    "\n",
    "All models have been evaluated on the validation set with comprehensive metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "### Completed âœ…\n",
    "- Data loading and preparation\n",
    "- Target variable creation (3-class, binary, 5-class)\n",
    "- Train-test-validation split\n",
    "- TF-IDF feature extraction\n",
    "- 4 baseline models implemented and evaluated\n",
    "- Model comparison and visualization\n",
    "\n",
    "### To Be Implemented ðŸ”œ\n",
    "- **Advanced Feature Engineering**:\n",
    "  - Word embeddings (Word2Vec, GloVe)\n",
    "  - Character n-grams\n",
    "  - Metadata features integration\n",
    "  - Sentiment scores (VADER, TextBlob)\n",
    "  \n",
    "- **Advanced Models**:\n",
    "  - XGBoost/LightGBM\n",
    "  - Neural Networks (MLP)\n",
    "  - LSTM/GRU for sequence modeling\n",
    "  - Transformer models (BERT/RoBERTa)\n",
    "  - Ensemble methods\n",
    "  \n",
    "- **Hyperparameter Tuning**:\n",
    "  - Grid search / Random search\n",
    "  - Bayesian optimization\n",
    "  - Cross-validation\n",
    "  \n",
    "- **Final Steps**:\n",
    "  - Best model selection\n",
    "  - Test set evaluation\n",
    "  - Model persistence\n",
    "  - Production pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
